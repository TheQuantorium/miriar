% Created 2023-02-09 Thu 09:25
% Intended LaTeX compiler: pdflatex

\documentclass{extreport}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{mathptmx}
\usepackage[T1]{fontenc}
\usepackage[
    type={CC},
    modifier={by},
    version={4.0},
]{doclicense}
\author{Sam Brew}
\date{\today}
\title{Miriar Specification}
\hypersetup{
 pdfauthor={Sam Brew},
 pdftitle={Miriar Specification},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.0.50 (Org mode 9.6)}, 
 pdflang={English}}
\usepackage{biblatex}

\begin{document}

\maketitle
\tableofcontents
\vspace*{\fill}
\noindent Please note that this is a living document, which will evolve and change to reflect the current principles and systems of Miriar. You can find the latest version of this document \href{https://github.com/TheQuantorium/miriar/releases}{here}. The version of this copy is \textbf{v0.1.0}.
\doclicenseThis


\part{Introduction}
\label{sec:org14ec0e7}

Blockchain was one of the greatest innovations in the history of decentralisation, and one that has spurred on a wealth of further innovations in the decentralisation of finance and organisation more broadly, to the extent that it has become the modern bedrock on which the ecosystems of today's 'web3' are built.

However, it is seldom that anyone has questioned this foundation, seldom that anyone has stopped to ask if it works well enough, or if there is a better alternative. As the Kolaris and Lykros systems have proven, there very often is, especially in the realms of storage and computation, yet these protocols leave some things to be desired. When we speak of decentralisation, it is clear that no single protocol can command everything perfectly, simply because, if our goal is to decentralise \emph{everything}, we would need to have a system that works for everything. This is, quite evidently, impossible.

So let us begin with Kolaris. That system is one that facilitates the decentralised storage and computation of data on a large-scale network, but one of its core problems is also one of its core advantages: it has no consensus mechanism. Among Kolaris nodes, there is no way to create a single source of truth, because the network itself works against the concept. Hence, to facilitate the one-hop routing that makes it so efficient, Kolaris must shell out to \emph{directory nodes}, who can only be trusted through the \emph{node-level redundant} system, which is by far the weakest point of Kolaris. A certain number of nodes, elected through a cryptographically secure and random process, are made to produce signatures continually to verify a node's space on the network. These signatures must be continually provided for them to be remotely trustworthy, yet this system is not fault-resistant. To shut down a Kolaris network, one would only need to shut down a percentage of the network equivalent to the percentage of a single node's NLRs who would have to go offline for a signing quorum to be impossible. Such an attack would prevent continuing signatures, thereby spreading through the network like wildfire and rendering all nodes suddenly unauthenticated, paralysing the network with no clear solution --- this is known as a domino attack.

The second problem raised by Kolaris is that of addressing: one of the most regrettable concessions of Kolaris, which itself aims to largely replace blockchain's uses beyond finance, is that it must use a blockchain to achieve secure addressing, since there is no way to intrinsically link an address like \texttt{google.com} to the node that stores that website's content.

These two problems, while they may seem separate, have a common solution: an append-only, decentralised ledger. At first thought, this might seem to be a blockchain, but this is exactly the first thought that has so paralysed our society to be dependent on that very technology. To consider our options intelligently, we must move beyond this first thought and consider: how might we best create an append-only, decentralised, fully replicated ledger on Kolaris?

This system is called \emph{Miriar}, and it not only solves these two problems, thereby eliminating the most dangerous attack that can be levelled against a Kolaris network, it also provides a mechanism for the payment of funds at an inbuilt level to Kolaris. In short, this system will potentially enable the total replacement of blockchain, in almost all its current uses.

\part{Specification}
\label{sec:orge7887dd}

\chapter{System Properties}
\label{sec:org6c6d895}

Before we dive into how a system like this can work, we need to first consider exactly what its properties should be. Unlike Kolaris, there should be \emph{total} replication of data across the network, except perhaps for sharding where reasonable, since bribery should be only feasible through an attack in which more than half the nodes on the network are malicious (i.e. a 51\% attack).

Further, like Kolaris, there should ideally be a strong distinction between users of the network and nodes of the network, allowing users to register data on the ledger without necessarily storing it themselves, for a fee perhaps.

The remaining properties are largely similar to those of a blockchain: the ledger should be append-only and totally immutable once set, there should be some mechanism by which consensus can be ensured on the correct ledger state, and the ledger should be able to accept arbitrary data, such as transactions, routing entries, or addresses. Beyond this however, it is the full intention of the author to make this system as environmentally-friendly as possible by reducing its power consumption where possible. With Bitcoin mining using more energy than the entirety of the Netherlands in a single year at this point\footnote{See the Cambridge Bitcoin Electricty Consumption Index, available at https://ccaf.io/cbeci/index/comparisons.}, to ignore this problem is to restrict the network's growth. At a purely cynical level, if a protocol is to become as widely used as possible, then cutting out those who believe climate change to be a key issue, whose numbers grow daily, is simply foolish. From an idealistic standpoint, it is our duty in designing these protocols to do our part for the ongoing environmental security of the human race, and for the protection of our planet and ecosystems.

Finally, as has been made clear by numerous economic analyses of blockchain technology, there must be a cost to the creation of new entries on the ledger to eliminate counterparty risk, lest the cryptographic security of the system have no tie to any real-world value: in Bitcoin, this is through the cost of the proof of work consensus algorithm, hence, there must be some mechanism that enforces a cost to creating a new entry (importantly, this cost simply needs to \emph{exist}, and a lower cost does not necessarily mean a less secure ledger at all, merely one that will likely have many more entries).

\chapter{Propagation and Gossip Protocols}
\label{sec:org9c2aa3a}

Most decentralised architectures have some kind of gossip protocol, through which nodes broadcast information to each other. This is generally a subordinate component of blockchains, but what if we made this central? What if \emph{this} became the core of our consensus mechanism?

Since we can build atop Kolaris, we in fact already have access to a perfectly structured gossip system: all nodes know their PLR groups, and their NLRs, and can therefore keep data in sync with them. In fact, if we consider units that use Lykros \emph{patch versioning}, whereby many writes are allowed simultaneously, before being grouped into epochs, data is already kept in sync by PLRs in this way. So what if we applied that exact system to a ledger?

By extending the PLR propagation mechanism such that \emph{all} nodes propagate information to all their PLRs and NLRs, with no fear of redundant propagation, then information entering the network at any point would be practically guaranteed to arrive at a node, since any given node will be told about it by their NLRs and PLR groups. Assuming the absolute minimum number of PLRs, and a number of NLRs \(C\), in accordance with Kolaris notation, along with network dimensionality \(D\), the number of nodes from which a single node would receive information is

\begin{align*}
2D + C. \tag{2.1}
\end{align*}

In a three-dimensional network requiring twelve NLRs per node, therefore, the smallest possible number of nodes from which one node would receive information is eighteen. Preventing transmission to any single node is therefore almost impossible once the propagation begins. And, of course, nodes that are offline can simply ask their PLRs and NLRs when they come back online what they missed: duping a node at this time would be exceptionally hard, especially since all further communications would have to be controlled as well, or else incoming signatures would appear invalid (signatures in propagation will be addressed in a moment).

TO better understand this system, let's say Alice submits a new \emph{entry} to the ledger by contacting Bob, a node, who is denoted \(N\). Denoting Bob's NLRs as \(^nN\) and his PLRs as \(^nP\), Bob would transmit this information to the set of nodes

\begin{align*}
N_G = \{ ^0N, ^1N, ..., ^CN, ^0P, ^1P, ..., ^{2D}P, ..., ^nP \}, \tag{2.2}
\end{align*}

where \(n\) is the total number of PLRs Bob has, which will necessarily satisfy \(n > 2D\) (since Bob will have a minimum of \(2D\) PLRs by simple geometry, but if he is larger than some of them, he will have more).

Every single one of these nodes in Bob's \emph{propagation set}, denoted \(N_G\) (\(G\) for gossip) will then continue this propagation. It is important to note at this point that the propagation set is \emph{bidirectional}, meaning it is also the set from which a node will receive a new entry \(\epsilon\). Depending on a node's position in the network, it may receive from a subset of its propagation set, or from all of it. A node receiving from \(N_H\), where \(N_H \subset N_G\), should then propagate to the set \(N_G \setminus N_H\). In other words, if a node's propagation set is 18 strong, and it receives from twelve of them, it should propagate to the other six.

Since both NLRs and PLRs are involved in this, and since NLRs are elected randomly, it is effectively impossible to theoretically model the spread of information through the network --- this would only be possible with full knowledge of the NLRs of each node (although this information would be available in the ledger, and therefore modelling is certainly possible in the real-world).

Due to this difficulty of modelling, it is regrettably infeasible to enforce propagation, since, if one does not receive information about an entry from a particular node, it may simply be because that node has not heard about it yet, rather than a deliberate withholding or the like. This can be worked around, however, through a broader mechanism that ensures that nodes do indeed participate in this whole system, which will be discussed later. Even so, if some nodes choose not to store the ledger or propagate information, this is likely fine due to the sheer size of the propagation sets and the level of redundant propagation.

What can be enforced however is \emph{correct} propagation: if a node receives a clearly invalid entry from another node, it should report that node, and all nodes must sign everything they propagate to other nodes. That is, the payload sent to the propagation set \(N_G\) is

\begin{align*}
\mathrm{sign}_{N_{SK}}(\epsilon), \tag{2.3}
\end{align*}

allowing forcible eviction by NLRs on this basis to be clearly marked (since the node \(N\) has provably signed an invalid entry in this case). Using this process for invalid forcible eviction would require all the NLRs to be in collusion against this node (and, if they are, they could simply evict it under the usual Kolaris protocols).

\chapter{The Ledger}
\label{sec:orge05d024}

Given this propagation protocol, we must now outline a system to store an actual append-only ledger system. This can be based suprirsingly effectively on Lykros' system of \emph{patch versioning}, originally designed for units in Lykros systems that have a large quantity of concurrent writes, which is incompatible with the \emph{linear versioning} system, in which each new version of a unit should sign the previous one. If, however, there are three new versions that arise simultaneously, only one would be able to become the next version, and the others would have to be rejected until they re-signed the one that was accepted. This is the exact same problem as arises in the storage of a ledger, and, pleasingly, the same solutions from the patch versioning system hold.

\emph{It is strongly recommended that the reader familiarise themself fully with the Lykros patch versioning system, before returning to this paper.}

The best way to explain how Miriar handles propagation and storage is perhaps an example. So, let's say that Alice wishes to submit a new entry to the ledger. Once she has a valid entry \(\epsilon\) (the form of which will be explained later), she should submit this to some set of \emph{intial gossip nodes}, who will begin the process of propagation through the network. By submitting to more nodes, there is of course a lower chance of censorship (since really only the nodes in that initial propagation set would be able to choose to censor). Once these nodes have validated Alice's entry (a process to be described later), they will pass it on according to the algorithm described in the previous section, while simultaneously adding it to their own record of the current \emph{live group} (the list of entries that have not yet been sorted into new epochs), which is a set \(\mathbb{L}\), ordered numerically, so that each entry has a defined place (eliminating issues of consensus with respect to order). They should then compute \(H(\mathbb{L})\), where \(H(x)\) is a hash function, and run this through the epoch creation function \(E(x)\), which will return a boolean as to whether or not this hash should lead to the creation of a new epoch. Whereas Lykros would typically decide epoch creation on the basis of when the live group has filled up, Miriar does this algorithmically, eliminating any issues of consensus, since, provided nodes have the same entries, they will necessarily create the same epochs (the matter of when they do \emph{not} have the same entries will be dealt with shortly). (Using the original size-based epoch creation algorithm proposed in the Lykros paper would almost certainly lead to many more conflicts, while also allowing the system to brute-forced much more easily, making it easier for attackers to force conflicts, potentially leading to catastrophic network forking in extreme cases.)

For clarity, once the entry \(\epsilon\) has been validated, a new epoch will be created from the current live group, with it, if

\begin{align*}
E( H( \mathbb{L} = \{ ..., \epsilon, ... \} ) ) \tag{3.1}
\end{align*}

evaluates to \texttt{true}. If this does happen, the nodes that find it so will propagate a second message to their propagation sets with the payload

\begin{align*}
\mathrm{sign}_{N_{SK}}(\{ H(\mathbb{L}), \mathbb{L}_I \}), \tag{3.2}
\end{align*}

where \(\mathbb{L}_I\) is the set of entry identifiers (each entry \(\epsilon\) has a unique identifier), allowing the live group that created this epoch to be reconstructed by any other node who knows all the entries that have been submitted recently. This is to prevent the issue of epoch conflicts, where two entries \(\epsilon_1\) and \(\epsilon_2\) may both trigger \(E(x)\) to signal epoch creation, on slightly different live groups, simply due to the time lag. Essentially, it is entirely possible that these entries might be submitted 'at opposite ends of the network', and they would have to meet 'in the middle'\footnote{For clarity, since the propagation set includes the NLRs, which have no geometric relation to one another, this is impossible. Also, PLRs ar enot next to each other, but a fixed geometrical distance from one another. While one could hypothetically model the course of propagation across a network, it does not do well to think about this for too long, especially in more than three dimensions.}, by which time there may well be two conflicting epochs. Nodes would be made aware of this conflict by the two conflicting epoch creation messages of the form in Eq. (3.2), and they can therefore reconstruct both live groups to resolve the conflict.

Of course, this assumes that all nodes involved have knowledge of all the entries in these live groups, which they naturally would, since epoch creation messages come \emph{after} entry propagation messages, but the pathways are the same. In essence, barring major network failure, the messages about the entries will always come before the messages about the new epochs (or at least within a short time of each other). Of course, network failures are inevitable, but the level of redundancy of the propagation algorithm makes this far less of a concern, if any. If a node does receive an epoch creation message with entry IDs it doesn't recognise, it should wait a moment to see if it receives them imminently, or then query the nodes in its own propagation set to see if they know about it. If they don't, it should dump the message as invalid. (However, of course, it would only be propagated to that node in the first place if one of the nodes in that node's propagation set believed it to be valid, implying they knew of all the IDs, so this situation should never occur unless some nodes are performing \emph{unchecked propagation} --- this is why they sign everything they propagate, to provide evidence on which their NLRs can evict them from the network for such behaviour).

Assuming for a moment that no conflicts arise, this system will allow continuing propagation of entries throughout the network, while steadily accumulating more and more entries in the live group, until new epochs are created on the basis of the \(E(x)\) function. To understand the security of this ledger, one must understand the form of \(\epsilon\). Given a payload \(\phi\),

\begin{align*}
\epsilon = \{ \mathrm{sign}_{U_{SK}}( \{ \epsilon_I, v, i, H(\mathbb{E}_2), \phi, \beta, \tau \} ), \eta \}, \tag{3.3}
\end{align*}

where \(\tau\) is a series of data that satisfy the network's restriction factors, and \(\mathbb{E}_2\) denotes the second-to-last epoch. A unique identifier for the entry \(\epsilon_I\) is also noted here, and new entries should only be added to the ledger if there are none with the same ID already registered, thereby preventing attacks whereby a malicious user resubmits a previous entry multiple times. In this notation, \(\mathbb{L} = \mathbb{E}_0\), and the subscript number counts back from the most recent, where each epoch itself is the set of entries within it. Of course, \(E(H(\mathbb{E}_n))\), where \(n > 0\), must output \texttt{true} (otherwise it would not be an epoch). \(v\) and \(i\) represent details regarding the \emph{validation mechanism} this entry should use (which will be discussed later), \(\eta\) is used for linking to the rest of Kolaris, and \(\beta\) is used for \emph{bundling}, which enables entries that are dependent on other entries. These mechanisms will be discussed throughout the paper.

\emph{Note that the \(\mathrm{sign}\) function is used here as in the Kolaris specification, meaning the signature, combined with a timestamp and the original data being signed. Here, the signer is \(U\), the user who submitted the entry to the network.}

Of course, we must assume in the design of a system like this that conflicts \emph{will} indeed occur, but, following the principles of Lykros and Kolaris, whereby the need for consensus is either eliminated or minimised, since clients can interpret the actions of disparate nodes on their own, any conflict resolution mechanism must be totally transparent and unambiguous. In general, there are two types of general conflicts possible in this system: there can be censorship, or \emph{epoch divergence}. The former, in which some nodes are not informed about certain entries, would be \emph{extraordinarily} difficult to successfully pull off, since it would require preventing not only the initial entry packets, but also the later epoch creation packets, along with all subsequent packets, since they would be clearly signing an epoch that these nodes who are victims of the censorship would not be aware of. Due to the sheer size of the propagation set for every individual node, the level of network control required to pull of an attack like that would be effectively 50\% of the network, and, even so, if a user submits their entry to even one node not controlled by the attacker, the attack would likely be mitigated. Further, such an attack would be visible from the very fact of the network's divergence in this manner.

The second kind of conflict is more natural, and one that poses a genuine threat to a network like this: if two entries do, as mentioned before, simultaneously trigger epoch creation given the same live group, it is possible that two conflicting epochs may develop at the same time. In this kind of conflict, there are 3 subtypes that are all addressed slightly differently in Miriar:

\begin{enumerate}
\item Two or more entries simultaneously create two or more conflicting epochs with only one entry different, and the same number of entries;
\item Two or more conflicting epochs arise in which one has more entries in it than the other;
\item Two or more \emph{sets} of \emph{multiple} conflicting epochs arise, an exacerbated version of either case (1) or (2) where further epochs have developed on both sides of the divide before the conflict has been resolved.
\end{enumerate}

Of these, case (2) is the easiest to solve, as we can just have nodes prefer the longer epoch in terms of the number of contained entries, taking all the entries in the rejected epoch that were not in the accepted one, and adding them back to the live group. \emph{This} is the reason Eq. (3.3) diverges from the typical Lykros patch versioning scheme, in which the most recent epoch \(\mathbb{E}_1\) would be signed: since this epoch is considered constantly under possible contention in the case of epoch conflicts, new entries should include the second-to-most-recent epoch's hash instead.

In case (1), the length-based resolution mechanism is unavailable, so we can simply go with whichever of the conflicting epochs has the highest numerical hash value, since there is no reason to prefer one over the other, unlike in case (2), where we should prefer to displace fewer entries back into the live group. Importantly, in either case, since some entries will be placed back into the live group, it is entirely possible that the very process of conflict resolution will itself create a new epoch on top of the accepted one. It is this kind of complexity that makes such distributed processes so difficult for humans to model internally, although, based on the resolution protocols defined in this paper, general agreement should be able to be reached on everything up to the second-to-most-recent epoch at all times. Importantly, the usual propagation mechanism is violated when conflict resolution is performed: a node who has resolved two epochs should notify \emph{all} the nodes in \(N_G\) about this resolution, rather than just those whom it has not ehard from yet. Such nodes can be visualised as those `in the middle of the network'. Since all this conflict resolution creates a \emph{volatile epoch}, when users query the network for some part of the ledger, nodes should only respond up to the last non-volatile (verified) epoch.

Case (3), however, breaks this pattern, since there will be more substantial epoch conflicts, such that even the second-to-most-recent epoch itself comes into contention. Importantly, maliciously triggering such a conflict would be \emph{exceptionally} difficult, as it would require brute-forcing the epoch creation function (e.g. to get a hash that conforms to some particular regular expression or mathematical inequality), while providing restriction factors for every one of the attempted entries, all while trying to keep up with the current live group (which generally renders it infeasible to force an epoch creation deliberately). Further, the natural occurrence of such conflicts would be minimised by the size of the propagation set for each node, since the network is interlinked for propagation not only through the geometric nature of the PLR system (which could create the effect of conflicting epochs 'meeting in the middle'), but also through the NLRs, which break this geometricity, thereby minimising the risk of one of the conflicts in case (1) or (2) becoming so much more severe as in case (3). However, there are two actions that can be taken to drastically reduce the risk of an event like this: the first is to have more stringent restriction factors to reduce the rate at which new entries are submitted, and the second is to set a lower bound for the number of entries in a single epoch, preventing the 'freak' case of one normal epoch being followed by another with only two entries, or similar. These measures, when combined, will force the network to reject any very small epochs, and wait for a larger number of entries to join the live group, during which time propagation will continue at the same pace, allowing conflict resolution.

If all these measures fail, which, to be frank, is likely in the longer-term to happen at least once, then the usual conflict resolution procedures should be applied: go with the epoch chain that has the larger number of entries, or with the one with the higher sum of the constituent epochs' hashes. The rest of the entries, from the rejected chain, should be placed back into the live group. Since the Lykros patch versioning algorithm accepts a signature of both the most recent and the second-to-most recent epochs in new entries normally, Miriar accepts signatures of either the second-to-most-recent or third-to-most-recent epochs, meaning any entries in rejected epochs that had signed the latter may have been rendered invalid, since two more epochs have now been added, meaning the old epoch they signed is now out-of-range. Such entries should be themselves rejected from re-addition to the live group, and therefore they will be in effect removed from the ledger as invalid. In such cases, the clients who sent these entries should monitor the network to check whether or not their entries have been rejected, re-submitting them if necessary. Admittedly, this does make higher-layer networks designed for things like transaction speed rather challenging to build atop Miriar.

Importantly though, there are two parameters that can be tweaked in Miriar to minimise the risk of a case (3) conflict: \(\Gamma\), the most recent epoch that can be validly signed; and \(\Psi\), the oldest epoch that can be validly signed. Generally, when describing a Miriar implementation, one should describe these parameters as the network's \(\Gamma\)-\(\Psi\): for instance a network like that described here would have a \(\Gamma\)-\(\Psi\) of 2-3. The larger this range is, the lower the risk of entries being rejected altogether, and the further back it starts, the longer it will take for entries to be 'confirmed' in the ledger, but the lower the risk of a case (3) conflict (even changing these parameters to 3-4 would greatly reduce the risk of a case (3) conflict, since it would only be meaningful if not one, but two further epochs developed on top of a conflict).

The decision of which \(\Gamma\)-\(\Psi\) settings to use for a real implementation should be made based on empirical data, and implementations of Miriar in library form should be generic over this.

\section{State and Epochs}

As discussed so far, the ledger system is built on the idea of epochs creating emergent state, which is somewhat analogous to Ethereum's system of state, however, there, the state is appended to each block, taking up far more storage than in Miriar.

An interesting property of the Lykros patch versioning algorithm that underlies Miriar is the amount of data that need to be stored to be able to access the ledger. As established above, \(\mathbb{E}_\Psi\) is the oldest epoch that might be signed by a new entry, and signing an epoch refers to signing all the entries therein and the optimised `mega-patch' of how the ledger state will change as a result of this epoch, meaning all each node needs to store is the \emph{state} of the ledger (i.e. the state of all individual contracts) at \(\mathbb{E}_{\Psi - 1}\), with the mega-patches (i.e. optimised representations of how that state will change) for each of the epochs up to \(\mathbb{E}_\Gamma\), and clients will be able to read all non-volatile epochs in the ledger. The volatile epochs should be stored in the same way, but generally should not be queried by clients wishing to examine the current state of contracts.

What this means is that there is no need for nodes to store any old entries in the ledger, allowing the storage requirements of the system to stay incredibly lean. While the state of the ledger may well accumulate over time (as new accounts are added, or as new contracts do unpredictable things), the number of entries stored will remain generally within a small range. In practice, this should mean the amount of storage each node has to devote to the ledger is minimal.

However, these amnesiac properties raise unique concerns regarding the resolution of conficting states, not in the sense that would naturally arise, but in terms of the attack whereby one actor might generate their own alternative ledger state and claim it to be valid. First, it should be stated that, no matter how easy or hard this task is, it will be incredibly difficult for that attacker to propagate their alternative state to others due to the difficulty of creating new nodes, and the difficulty of having them be in propagation groups where their bogus states would be accepted. Since these nodes would not be propagating new entries, they would likely be quickly removed from the ledger, meaning a 51\% attack on this basis is incredibly tricky to mount: the network's `immune system' of sorts (through the NLRs) will neutralise most such threats.

However, we can easily build in a mechanism to make creating alternate ledger states more difficult, thereby making such attacks considerably harder, and reducing the risk of a network fork (which, although a legitimate action in a distributed network, is generally catastrophic for public faith in the underlying protocols). This can be done by creating a new representation of the ledger,

\begin{align*}
\mathbb{V} = \{ \{ H(\mathbb{E}_\Gamma), H(\mathbb{E}_{\Gamma_P}), \mathbb{E}_{\Gamma_\tau} \}, \ldots \},
\end{align*}

where \(n\) is the number of entries in the ledger. All this is is, for each epoch since \(\mathbb{E}_\Gamma\), the hash of all the entries in that epoch, together with the hash of the mega-patch, and then the list of all the \(\tau\) values for each entry in that epoch. In reality, this would be arranged so that entries would be matched up with the epochs which their proofs sign, allowing any client to go through all the micro-proofs and validate them. This requires an in-depth understanding of the nature of these proofs, which are defined when signing the epoch \(\mathbb{E}_n\) as

\begin{align*}
\tau = \{ H_P(\epsilon_H, \mathbb{E}_n \mathbb{E}_{n_P}, \tau^\star), \epsilon_H \},
\end{align*}

where \(\epsilon_H\) is the hash of the other elements of the entry \(\epsilon\) (what this includes is self-explanatory). Again, this is all fairly simple: each proof is just the hash of the hash of the entry data, the hash of the entries in the previous epoch and its mega-patch, and some random number \(\tau^\star\) needed to make the hash satisfy the proof requirements. This way, when given \(\mathbb{V}\), a user can validate all the incuded micro-proofs, ensuring that they are indeed valid, hence forcing any attacker to create a large number of micro-proofs, which should be created with a hash function \(H_P\) that is non-parallelisable, if possible, increaisng dramatically the cost of launching an attack like this. Add to that the fact that they don't know when each of their bogus epochs will be, and they begin to run into significant problems. And, even if they do manage all that, they would have to control vast swathes of the network first to be able to do this, meaning they would have to control, in mature networks, 51\% of the storage and computation power of the entire network (since this is what is required to control a Kolaris network), and all with nodes in exactly the right places.

Hence, Miriar is resistant to attacks that create entirely conflicting altnerate states, and requires minimal storage from nodes, who must only store the entries for the epochs between \(\mathbb{E}_\Gamma\) and \(\mathbb{E}_\Psi\), and only one full state. Adding to this a validation layer to create the aforementioned attack resistance, and Miriar should still be one of the lightest ledger systems in history, minimising the likelihood of nodes not storing critical data, while also ensuring true decentralisation by minimising the barriers to entry of running a node.

\chapter{Incentive Systems}
\label{sec:org66e610c}

Incentivising nodes in this network to store our ledger is a tricky business, because, in Kolaris, nodes are duly compensated for their efforts in storage and computation, according to the Kolaris pricing mechanism. A Miriar ledger would grow, however, and would need to be stored by every (or almost every, for practical purposes) node on the network, without compensation. Adding the cost of storing this ledger to the Kolaris pricing equation introduces both unnecessary complexity and implies that nodes should be compensated in some way for this action, which only applies according to economic logic if they are rendering a service to someone else. In reality, they are rendering a service to the entire network, and to themselves --- as in an empty cinema there is no marginal cost to admitting one extra moviegoer (until the theatre runs out of seats), there is no loss from one node not storing the ledger, all that matters is that the vast majority do.

One way of incentivising this is to have storing the ledger provide a clear benefit to nodes, and, taking inspiration from the Bitcoin protocol's mining design, this can also provide the solution to the problem of minting new tokens in a cryptocurrency that would hypothetically run on this network. Imagine some process by which nodes would be elected to receive a newly-minted reward in this token, but claiming it requires them to have knowledge of the entire ledger, thereby incentivising them to store it all. This is certainly not perfect, but it is better than nothing by a long way. (The main problem with this is that nodes might simply ask some other party to store the ledger and pay them to complete this process instead. If enough nodes pay the same entity to do this, that entity would gain censorship-level power.)

Proof of work is this mechanism in blockchain, but this differentiates miners from so-called `light clients', which participate in the blockchain without performing much, if any, validation (the extent to whcih such clients simply leech off the network differs from blockchain to blockchain). Miriar makes a different distinction, based on Kolaris: all nodes both store the ledger and perform the role analogous to mining, and are therefore eligible for a reward of new tokens, while users of the network who are not Kolaris nodes are neither eligible for rewards, nor do they have to store the ledger, in whole or in part (though they may wish to cache parts of it, or hashes of parts, for speed).

It may be tempting to consider devising some extraordinarily elaborate proof process here to tie everything together, to trade off the simplicity of the rest of Miriar with something incredibly complex here, but this is actually not required at all. As it turns out, the key difference between Miriar and existing systems that try to achieve the same ends (like blockchain) is its use of the Kolaris network as a structured propagation protocol, which, when combined with Lykros patch versioning, practically eliminates the risk of non-consensus situations in the long-run. Hence, we can use simple random election to decide who gets the reward.

In essence, every time a new epoch is created, the hash of the entire ledger up to that point, including that latest epoch, will have zero added to the end of it, and that will be run through the network hash function to produce a point. Then the same will be done, but for one, and then for two, etc., until \(Y - 1\), such that \(Y\) rewardees are elected automatically. Of course, since this process is fully deterministic on the immutable state of the ledger up to that point, it does not need to be 'performed' by any centralised entity, and that is the exact point: any node wanting to know whether they have received a reward must find out for themselves by executing this computation (which requires knowing the entire ledger), and then, depending on the parameters of the network, they may also have to satisfy certain restriction factors (e.g. such as a proof of work, or even, say, submitting a picture of themselves with antlers on), before a claim they then enter into the ledger can be considered valid. Such a transaction entry should be examined by each node propagating the ledger to check that the network hash of the ledger up to the last assured epoch, when the declared number is added, goes to a point in the range of the node receiving the claimed reward.

Note that this implicitly means that rewards can only be claimed in the epoch directly after they are generated, so the incentive for each node to store the entire ledger is greater for shorter epoch times, improving this aspect of Miriar's security.

\emph{Note: to ensure that \(\mathbb{V}\) is also being stored, this should be included in the hashes used to assign rewards.}

\chapter{Addressing}
\label{sec:org9928c7d}

As alluded to earlier, one of the main reasons Miriar is useful to Kolaris itself is to solve the problem of linking human-readable addresses (HRAs) with their corresponding root index units (RIUs), since there is nothing that intrinsically links, say, \texttt{google.com} with the point in Kolaris network-space at which the \texttt{index.html} file of that website is stored. However, an append-only ledger can very easily solve this if tuples of addresses with their corresponding RIU identifiers are registered as entries on that ledger. Importantly however, just submitting such a tuple would not be a good idea, since any nodes who wished to control that HRA for themselves could submit a conflicting tuple. Such \emph{content-based conflicts} in the live group can only be validly resolved by rejecting both nodes, which may lead to a future race condition between different users who wish to control a specific HRA. The best way to mitigate this is through a specialised requirement on the \(\tau\) component of such entries, such that the more complex proof of work always wins out in content-based conflicts between addresses. Although this does not provide any additional protection for the address in the long-term (since, once immortalised in the ledger, it cannot be removed), it does prevent nodes who propagate this entry from thinking they would like to control that HRA themselves, since they will presumably be unable to produce a proof of work as lengthy as that which the original submitting user has created before the next epoch is created.

However, there is one problem not addressed by such a solution: the unlikely event of an epoch conflict that a malicious node who also wants control of this HRA coudl take advantage of. To be clear, the time it would take to force an epoch conflict should, due to the restriction factors involved, be far longer than the time taken to produce a more lengthy proof of work, but, nonetheless, it is \emph{plausible} that an epoch conflict could naturally arise with conflicting address claims in each epoch. This would be almost impossible to orchestrate as a node, since, if Alice the node wishes to take control of an HRA only after she has seen it when it was propagated to her, she would be on the wrong side of the epoch conflict definitionally (since she has already seen the alternative and registered it). Only pre-propagation observation would enable this, and again, only with a very slim chance of working. However, if the epoch with Bob's original claim to some HRA is shorter than the one with Alice's, even if Alice's claim has a simpler proof of work, it will be accepted, since Bob's will be booted out into the live group, where it will suddenly conflict with Alice's 'established' claim.

The solution to \emph{this} is to publish an address claim in two stages: the first is an opaque claim that links the RIU's address to an encrypted version of the HRA, and the second is a link to that (by its index in the ledger) which provides the decryption key and the actual HRA, meaning anyone who is searching for a certain HRA can simply search the ledger for it, and then pop back to the earlier link with the RIU. The first tuple can be confirmed to be validly linked to the second tuple by the fact that it contains the HRA, encrypted with the key revealed in the second tuple. The original claim is the first tuple, meaning that anyone who submits a first tuple of their own later, thinking the address to be unclaimed, will unfortunately be in for a nasty surprise later, since the following second tuple will, even if it comes after their first tuple, or even their second tuple, be preferentially valid. In essence, regardless of the order in which the second tuples come, the one that points to the earliest first tuple should be considered valid. In order to prevent an attack whereby someone hoards addresses, waits for them to become popular, and only then reveals their second tuple, we shall mandate that the second tuple must come within three epochs of the first (though this number could be changed).

Hence, the first tuple should be of the form

\begin{align*}
\{ \mathrm{encrypt}_k(a), \mathrm{sign}_{r_{\mathfrak{p}_{SK}}}(r) \}, \tag{5.1}
\end{align*}

where \(a\) is the HRA and \(r\) is the RIU's identifier (which of course includes its Lykros UKF implicitly, ensuring its contents cannot be forged). Note also that \(r\) is signed with the RIU's change permissions public key (the hash of which is in its unique identifier, see the Lykros specification for details), preventing links to arbitrary RIUs that the author of such an entry as this does not actually control. The second tuple should then be

\begin{align*}
\{ a, i, k \}, \tag{5.2}
\end{align*}

where \(i\) is the index of the first tuple in the ledger.

Of course, both Eqs. (5.1) and (5.2) refer to the payloads of entries, which should be constructed into full entries according to Eq. (3.3).

The final point to be addressed in this system is the transfer of addresses, which can be achieved by providing a later tuple of the form

\begin{align*}
\{ i, \mathrm{sign}_{r_{\mathfrak{a}_{SK}}}(\mathrm{sign}_{r'_{\mathfrak{a}_{SK}}}(r')) \}, \tag{5.3}
\end{align*}

where \(r'\) is the new RIU, and \(\mathfrak{a}\) represents the \emph{addressing permission}, which should be added as an additional Lykros permission to any RIUs (this can be stored in the permissions PAMM). Any subsequent transfers should use the \(r'\) keys, thereby removing the ability of the original owner to control further transfers. Importantly, the process by which users evaluate an HRA to a RIU is as follows:

\begin{enumerate}
\item Search the ledger for the HRA itself, finding all 'second tuples' that link to a first tuple;
\item Of these, follow the earliest index (of the first tuple), using the given key \(k\) to decrypt the HRA in the linked entry. If this produces the same HRA, it should be considered valid (note that any second tuple that pointed to an entry it couldn't decrypt validly should be rejected in propagation);
\item Search the ledger for the index of that first tuple to find any re-assignments of the address, following these continually until there are none left.
\end{enumerate}

Importantly, step 3 will repeat for every re-assignment, meaning high-churn addresses will take longer to resolve on the network, thereby disincentivising their use (since their value will implicitly depreciate on any re-assignments). Still, this operation is extremely fast for a locally-known ledger.

Hence, Miriar enables the reliable linking of human-readable addresses to points in a Kolaris network, thereby enabling truly secure development of websites and apps in a totally uncensorable manner.

\chapter{Rate Limiting}
\label{sec:org9f56f8d}

As mentioned several times already, the restriction factors described in the \(\tau\) parameter of Eq. (3.3) are essential to the success of a Miriar system, primarily to limit the creation of new entries, minimising the risk of the case (3) conflicts described in section 3. However, more generally, it can be useful to restrict the creation of new entries to make propagation a more manageable endeavour: any distributed system based on a gossip protocol will be far easier to manage if it has a lower throughput. Finally, this solves the problem of eliminating counterparty risk by creating a marginal cost to each new entry in the ledger. Importantly, this means these restriction factors should be updated continually as time goes on and processing power improves, along with interest in a Miriar implementation.

Perhaps the most obvious mechanism for rate limiting is a proof of work, which is a somewhat ironically disregarded use for this algorithm, since it was originally intended for minimising email spam. In the same way here, it is being used for minimising spam to the ledger (which is advantageous in the longer-term, since it lowers the size of the ledger, and therefore the burden on nodes). At the time of writing, the Ethereum blockchan is around half a terabyte in size\footnote{TODO}, and the use of proofs of work can certainly help in a network of Miriar's scale to reduce storage requirements. Sharding, described later, also has major utility in solving this problem.

Importantly, the proofs of work required here should not be extremely computationally intensive for any sort of transaction, since it would be a little pointless to send a five cent transaction if the implicit 'transaction fee' is a dollar's worth of computation. For addresses, however, the proofs required should be more intensive. The exact restriction factors used in both systems should be subject to further research, and this paper will only outline the general need for them at this stage.

Finally, it is important to note, with regards to the issue of marginal cost, that a unique property of Miriar's use of proof of work is \emph{who} performs the operation: in nearly all blockchain systems, miners do, and there are significant economies of scale to this endeavour, meaning the marginal cost of creating a new entry implicitly declines over time. By contrast, Miriar has \emph{individuals} be responsible, and, while it is certainly plausible that services would arise to generate proofs of work more quickly (such a service would experience economies of scale similar to those of Bitcoin miners), for each individual, the marginal cost of adding a new entry does not change in any meaningful way, which, at a theoretical level at least, renders Miriar far more secure against large influxes of bogus entries intended to overwhelm the network. Whether or not this eventuates in practice is of little concern, since the restriction factors can be tweaked as necessary to keep entry throughput to a manageable level no matter how the economics turns out, but, hypothetically, Miriar should be able to achieve the same level of security against spam as proof of work blockchains with significantly lower requirements, which would be more amenable to execution on consumer devices, rather than gargantuan mining clusters.

\chapter{Routing}
\label{sec:org28e611f}

Extending on the applicability of Miriar to addressing in Kolaris, it can also in fact \emph{replace} a large part of Kolaris: directory nodes. Since Miriar requires nodes to communicate with their PLRs and NLRs, which are both known at all times, without needing to query DNs, a routing layer can be built on top of it. Provided the time for an entry to reach the 'safe' zone of \(\Psi\) epochs back is lower than the length of the Kolaris probation period \(P\), this is entirely feasible.

However, replacing the DN system with Miriar is not as simple as you might expect, since it also entails the total removal of NLR signatures, resigning NLRs solely to the role of accepting records of bad behaviour and exiling nodes, thereby preventing \emph{domino attacks}, wherein a portion of the network is attacked so as to prevent NLR quora from being able to be reached for some other nodes, meaning they will fall into a state of unverified limbo (since NLR signatures must, in Kolaris, be continually re-provided to ensure ongoing range validity), which can spread across the network, potentially leading to data loss and a mass exodus of nodes.

To understand how Miriar can solve problems like these, we need to first analyse exactly what the role of DNs is in Kolaris: in short, they store the value pair \(\{ N_{AC}, N_{RC} \}\) for each \(N\), indexed by the keys \(N_P\) and \(N_R\) (i.e. the addition and range certificates are indexed by the nodepoint and range). When indexing by a node's nodepoint, which will never change, it is impossible for the DN to return anything invalid, as explained in the Kolaris paper, provided 'colour of the sky' checks going back to the genesis node \(^GN\) are possible. It is when a client wishes to index by the \emph{range} of a node that problems arise, since it is eminently possible for a DN to provide the old range of a node if they wish to manipulate the network, which can even lead to their controlling large parts of the network, severely undermining Kolaris' security. This is mitigated traditionally by having the NLRs sign the range certificate (\(N_{RC}\)) of their node every probation period, meaning that, if the latest signature has not yet been provided, the node's range should be considered potentially unverified, and the node itself shoudl be considered offline. (This is what facilitates domino attacks.)

If we were to store all this in an append-only ledger, however, we might instead first store the key-value pair

\begin{align*}
\{ N_P, N_{AC} \}, \tag{7.1}
\end{align*}

since neither the nodepoint nor the addition certificate will ever change. Again, this mapping is infallible, and the addition certificate validates that whoever submits this entry to the ledger has the authority to do so (since it includes signatures from the addition chain and the parent node, which should be validated during propagation). This therefore allows indexing a node by its nodepoint without a DN, and, neatly, using a ledger that is distributed across the entire network for this means a client can contact any node and receive this tuple infallibly (since it will never need to be updated), spreading the routing burden across the entire network, rather than a collection of DNs.

As for the range, this is slightly more complex, since a node's range will change over time, as new nodes are added to its space, and as other nodes adjacent to it are removed, and it expands to fill some of their space. Traditionally in Kolaris, such changes would be captured by the \emph{range modification list} (RML), which, together with the signatures of the NLRs on it, makes up \(N_{RC}\), the range certificate. In a ledger system like Miriar, we can list each element in the RML as a separate entry in the ledger, with each one linking to the entry described in Eq. (7.1). Since such entries broadly fall into three categories: addition, exile, and voluntary removal, the forms of the entries for each are noted now. When a new node \(A\) is added that takes up some of the space of \(N\), the entry payload should be

\begin{align*}
\{ N_P, N_{R'}, A_P, ``addition\textrm{''} \}, \tag{7.2}
\end{align*}

which is the nodepoint and the new range \(N_{R'}\), which can be trivially validated in both propagation and later examination from \(A_{AC}\) (which is infallible due to the addition chain), which can be acquired by checking the ledger for \(A_P\), included here. We don't include \(A_{AC}\) in such entries to minimise the size of the ledger.

When a node \(A\) adjacent to \(N\) is exiled from the network by its NLRs, or when it voluntarily leaves, the entry payload for the adjustment to \(N\)'s range should be

\begin{align*}
\{ N_P, N_{R'}, A_P, ``removal\textrm{''} \}, \tag{7.3}
\end{align*}

which is the nodepoint of \(N\), the new range, and the nodepoint of \(A\), which should link at this point to a removal certificate previously submitted either by \(A\) (in the case of voluntary removal, according to Kolaris Eq. (6.3)), or by one of the NLRs of \(A\) (in the case of forced exile, according to Kolaris Eq. (6.4)).

Note that, in both cases of removal, the new range \(N_{R'}\) can be validated through the node removal algorithm, which can be trivially executed both during propagation, and during later inspection.

Importantly, entries of the form in Eq. (7.2), regarding the addition of a new node, should be submitted by the parent node \(N\) when \(A\)'s probation period is up, while those entries regarding the removal of other nodes, as in Eq. (7.3), are more complex: the removal of a node \(A\) will affect the nodes adjacent to it on the removal axis, of which there could be an arbitrarily large number, so they will each need to individually submit one of these certificates upon such a removal, linking through the nodepoint \(A_P\) of the removed node to the removal certificate that has been previously submitted. During the interim period when a removal certificate has been issued for \(A\), but not a range update certificate for \(N\), clients can simply run the node removal algorithm themselves to determine where to send their requests. Before the removal certificate of \(A\) has been published, it should be considered as still operational, but, if it is offline, requests should be redirected to its PLRs for the point in question, as usual.

Thus, we are able to use Miriar to construct an ongoing list of updates to ranges, which together effectively constitute a routing protocol. The problem of ranges in Kolaris is not so much that fake ranges might be issued as it is that a DN might provide a node who \emph{previously} occupied a certain range, although this is impossible if the entire ledger is known. Since all nodes store the ledger, the routing between compliant nodes is therefore infallible, with no NLR signatures needed. As for clients, very few of whom will likely store the full ledger, they would most likely depend on a series of nodes that they trust, although this itself opens them up to the possibility of fraud, so they should always check with random nodes, and ideally download the entire ledger themselves.

In terms of attacks that could be executed using this alternative routing system, a node who was queried by a light client about which node controls a certain point could provide an old range, they could suppress removal certificates, they could state that a node does not exist, but they could not provide an outright \emph{false} range, since a range should always be validated by checking it against the removal and addition certificates that have affected the original range in the node's own addition certificate. For example, if Alice the node begins by controlling some range, and then her range contracts due to some new node taking part of her space, and later some client asks her who controls part of her old range, she could feasibly say herself, and, without seeing the whole ledger, this client would know no better (which is why asking many uncorrelated nodes is critical). Of course, if a client first asks Alice who the nodes are that it should query in addition to her, this would be a little pointless, since she could respond with the addresses of other nodes she controls, giving the client a false sense of security.

In general, using Miriar as a replacement to Kolaris' DN architecture reduces the overall cost of running the network (since DNs are simply not required), removes the partial censorship power that DNs can hypothetically have in Kolaris if they collude with each other, and it mitigates entirely the possibility of a domino attack. The tradeoff is the loss of confidence for nodes that they cannot be lied to about routing, since the nodes they query may provide them with bad range information, which is why, especially in systems that have a large number of light clients, a hybrid system may be best: use Miriar for all routing information, allowing infallible routing without domino attack vulnerability, but also have DNs exist for storing the kinds of certificates that NLRs continually sign to validate the RML of a node, thereby allowing light clients to act with confidence about ranges, as in the traditional Kolaris architecture. If they find a node to be in limbo, they then begin querying the ledger itself to determine the state of that node, and large-scale domino attacks become impossible, since the 'limbo' state of not having continuing NLR certificates becomes one that only impacts a secondary layer of routing, rather than the foundations of the whole routing protocol. For large-scale networks, this hybrid approach is recommended (with Miriar also acting as a historical database of previous ranges, which is far more feasible than other approaches to this, which are left vague in the Kolaris specification).

\chapter{Propagation Validation}
\label{sec:org8c98352}

Existing distributed ledger systems have generally adopted one of two paradigms with regards to the execution of arbitrary computations in ledger validation: all must follow a single, pre-defined pattern (e.g. Bitcoin), or different types of entries may declare arbitrary validation algorithms, often called \emph{smart contracts}. In Ethereum, each block contains the 'state' of the network, and the fact that Miriar uses Lykros patch versioning provides a hint to how it might handle this: through \emph{emergent state}. Every entry in Miriar can mutate some hypothetical state that emerges from the fact that there are entries that have mutated it. For example, let's say Alice wants to hold \emph{a thing} in escrow before recording that it has been transferred to someone who was able to provide a proof of work first. Or, after thirty days, if no-one has solved the problem, the token defaults back to her.

In order to understand how we might implement this, we must first define what exactly is available to a Miriar \emph{script}, and how these are different from Kolaris computation scripts. Firstly, any Miriar script will be executed by every single node on the entire network at least once, meaning its output is effectively absolutely trustworthy, provided a 51\% attack has not been executed on the Kolaris network (which, as shown in that paper, would be unreasonably difficult). Further, Miriar scripts must always simply return a boolean value: either the entry should continue to be propagated, or it should not. Astute readers will note that this means \texttt{true} and \texttt{false} have different levels of verifiability: if a computation outputs \texttt{true}, then the entry will end up being recorded in the ledger, implying the the network agrees that it is valid. If it outputs \texttt{false}, then it may be stopped by the very first node, and therefore it would only be executed once (which is highly fallible). To get around this, if a node receives a computation from a \emph{client}, implying that it is responsible for beginning the propagation, then it should propagate \emph{regardless} of whether or not the script outputs \texttt{true}. But, if it outputs \texttt{false}, it should note this in its signed propagation to \(N_G\). If a client provides its entry to, say, three nodes to begin with, then this would mean it would be validated by 54 nodes, assuming a network where \(D = 3\) and \(C = 12\), which should be more than satisfactory. If an attacker wished to censor an entry, they would have to know where it was being delivered to first (which, with anonymous communication addresses, would require control of 51\% of the network, at least).

The next asset Miriar scripts have available to them is Kolaris itself: the Kolaris computation system is unsuited for many contract-style applications, and it generally works far better for performing arbitrary computations for a server backend, or the like. Through its verifiable computation system however, and especially when zk-SNARKs are added (the proofs for which can be conveniently computed through Kolaris itself, absolutely verifiably), Kolaris can act as an oracle for Miriar computations, simply requiring someone to execute the script required. If this script needs to be run only once, it is probably best for a client to do so, and to then enclose the certificate in their entry payload, but recurring scripts are perhaps best stored in Kolaris itself immutably, and then referenced for execution (since all this requires is a single request to a computation bridge, which can be performed by any of the propagating nodes or a client).

In the simple example of Alice's escrow system, however, very little of this machinery is required. First, she should write a script (in any programming language that can be satisfactorily containerised for execution, most likely by WebAssembly or the like, here in Rust) something like this:

\begin{verbatim}
#[miriar::main]
fn main(curr_id: Id, entry: Entry<Payload>, state: State) -> Result<(), (Id, String)> {
    // Some contracts may need access to more specific aspects of the entry, such
    // as its tau-proof, but here we just want the payload
    let entry = entry.payload;
    match entry {
        Payload::Begin { day, .. } => {
            if let Some((id, _)) = state.begun {
                return Err((id, String::from("contract already begun")));
            } else if day != Day::current() {
                // This is a self-blame, since this entry is objectively invalid,
                // without needing any reference to any others (it will be invalid
                // regardless of the state of the contract)
                return Err((
                    curr_id,
                    String::from("cannot retroactively/proactively begin the contract")
                ));
            } else {
                return Ok(());
            }
        },
        Payload::Claim { proof } => {
            if let Some(id) = state.claimed {
                return Err((id, String::from("reward already claimed")));
            } else if let Some(id) = state.terminated {
                return Err((id, String::from("contract already terminated")));
            } else if state.begun.is_none() {
                // Another self-blame
                return Err((curr_id, String::from("contract not begun")));
            } else if Day::current() > (state.begun.unwrap().1 + 30) {
                // We blame this on the beginning entry
                return Err((
                    state.begun.unwrap().0,
                    String::from("too late to make a claim")
                ));
            } else {
                if proof_is_valid(proof) {
                    return Ok(());
                } else {
                    // Self-blame
                    return Err((curr_id, String::from("proof invalid")));
                }
            }
        },
        Payload::Terminate => {
            if let Some(id) = state.terminated {
                return Err((id, String::from("contract already terminated")));
            } else if let Some(id) = state.claimed {
                return Err((id, String::from("reward already claimed")));
            } else if state.begun.is_none() {
                // Self-blame
                return Err((curr_id, String::from("contract not begun")));
            } else if Day::current() < (state.begun.unwrap().1 + 30) {
                return Err((
                    state.begun.unwrap().0,
                    String::from("too early to terminate")
                ));
            } else {
                return Ok(());
            }
        },
    }
}

#[miriar::payload]
pub enum Payload {
    Begin {
        amount: u16,
        day: Day,
    },
    Claim {
        proof: String,
    },
    Terminate,
}

pub struct State {
    /// The day on which the contract began, and the ID of the entry that began it, if
    /// indeed it has been begun yet.
    begun: Option<(Id, Day)>,
    /// The amount staked. This does not have an entry ID assoicated with it, because it
    /// doesn't need one: after the contract has been begun, no other entry can
    /// change this, or declare a different value. Conflicting `Begin' payloads
    /// will be identified without this.
    amount: u16,
    /// The id of the entry that claimed the reward, if the reward has been claimed.
    claimed: Option<Id>,
    /// Whether or not the creator has `terminated' the contract by taking the reward
    /// back, which is not possible after a claim (i.e. the contract will always
    /// either be waiting, claimed, or terminated, if it has been started).
    terminated: Option<Id>,
}
#[miriar::derive_state]
pub fn derive_state(
    prev_entries: Vec<(String, Payload)>
) -> Result<State, (Id, String)> {
    // If there have been no entries for this contract yet, then it hasn't been
    // started yet
    if prev_entries.is_empty() {
        return Ok(State {
            begun: None,
            amount: 0,
            claimed: None,
            terminated: None,
        });
    }

    // In Rust, this removes the first element of the list, allowing us to iterate over
    // the rest later
    let first_entry = prev_entries.remove(0);
    if let Payload::Begin { amount, day } = first_entry.1 {
        let mut claimed = None;
        let mut terminated = None;
        let mut begun = None;

        // First, handle possible errors that exist *without* this new entry.
        // This excludes the first entry because of the earlier `.remove()'.
        for (id, entry) in prev_entries {
            match entry {
                Payload::Begin { .. } =>
                    return Err((id, String::from("contract began multiple times"))),
                Payload::Claim { .. } => {
                    if terminated {
                        return Err((id, String::from("claimed after termination")));
                    } else if claimed {
                        return Err((id, String::from("claimed twice")));
                    } else {
                        claimed = Some(id);
                    }
                },
                Payload::Terminate => {
                    if terminated {
                        return Err((id, String::from("terminated twice")));
                    } else if claimed {
                        return Err((id, String::from("terminated after claim")));
                    } else {
                        terminated = Some(id);
                    }
                }
            }
        }
        // It is impossible to have `claimed && terminated == true` after this loop

        Ok(State {
            // We know these two values from the first `Payload::Begin' entry
            begun: Some((first_entry.0, day)),
            amount,
            // And we know these from looping over the remaining entries
            claimed,
            terminated,
        });
    } else {
        // If the first payload isn't starting things, something has gone wrong
        return Err((
            first_entry.0,
            String::from("first payload does not start the contract")
        ));
    }
}
\end{verbatim}

This may seem like a very substantial script, but in reality it acts very simply, and shows rather well how such scripts operate: there is always a \texttt{main} function, or some kind of entrypoint, which must take the current entry's ID, the entry itself, and the state of the contract thus far. There is then a function \texttt{derive\_state} to derive a \texttt{State} from \texttt{Vec<(Id, Payload)>} (i.e. a list of previous entry IDs with their corresponding payloads), which means the node running this script can iterate over all previous entries for this contract, plug them into that function, and then have a \texttt{State} to provide to \texttt{main}. This function encapsulates the idea of \emph{emergent state}: it quite literally emerges from all the previous entries. Importantly, the \texttt{derive\_state} function must be a \emph{pure function}, meaning, given the same ledger state, it must always return the same output --- this allows the state of contracts to be cached by nodes to simplify future executions.

As for the actual logic of the contract, it largely consists of ensuring that errors do not occur: most of the state derivation and entrypoint functions are spent making sure that we don't have things like claims after termination, or the like. Importantly, the \texttt{main} function very simply returns a \texttt{Result} (which, in this context, can be thought of a \texttt{bool}, where \texttt{false} has a \texttt{String} reason attached, and an \texttt{Id} to blame), indicating whether or not the new entry should be allowed through. The state derivation function has the same return type, but, on a success, will attach the \texttt{State} object. If this state derivation function is run against the ledger up to the latest stable epoch \(\mathbb{E}_\Gamma\), no such errors should ever occur, unless there has been a serious failure in previous propagation validations. However, if this is run against a volatile epoch, or against the live group, this blame mechanic becomes important, because it indicates which entries are mutually exclusive, and, in the live group, which ones should \emph{both} be removed. The exact conflict resolution mechanisms used here will be described in detail in a moment. Notably, the state itself includes the IDs responsible for each part of the state, allowing the main contract logic to blame other entries for any errors that occur. The blame mechanic denotes primarily mutual exclusivity: if one entry blames another, it cannot exist in the ledger with that entry, but either could implicitly exist on its own. Sometimes, there will be a \emph{self-blame}, which indicates that an entry is invalid no matter what the state of the ledger is. In the example code above, this will occur in cases such as when an invalid claim proof is submitted, or when the contract has not yet been started (since the only valid entry at that stage would have the \texttt{Begin} payload).

Once Alice has this script, she can make it into a contract by compiling it to something like WebAssembly\footnote{See https://webassembly.org.}, and then hashing it: this hash is the contract type (noted as \(v\) in Eq. (3.3)), and it would be idiomatic in human language to refer to contracts by the first few letters of their hashes (if there are conflicts, perhaps replacing them with dashes). Of course, this program needs to be accessible to all the nodes involved, and, since this is a fairly small and custom contract, it is most likely that Alice would simply transmit this program together with her first entry, as later claimants would. Of course, her first entry would be of the type \texttt{Payload::Begin}, defining the amount of the reward and the current day. Notice that, if she does not specify the correct current day, nodes will reject this (of course, this should be in a pre-agreed timezone, such as UTC). It is generally recommended that contracts like this, that access the day, be executed early in the day (so propagation does not slip over into the following day, thereby invalidating the entry). Using hours and allowing leeway is probably a better implementation here.

Importantly, it may not make sense as to why the main function does not return the new state, but remember that all entries for this contract must have payloads that can be deserialized into \texttt{Payload} (if they cannot be, the entry will be rejected before even getting to the script), and therefore the very fact that the new entry is accepted \emph{mutates} the state for the next execution, since the next call to \texttt{derive\_state} will include it.

Note also in this specific case that we are assuming that this \texttt{amount} is entirely hypothetical: there is no linking to any currency system or the like, though cryptocurrency in Miriar will be dealt with later.

There is, however, one very important problem with this system. If the validity of new entries is computed on the state of the ledger up to \(\mathbb{E}_1\), the most recent epoch, it is entirely possible that another entry will come into the live group that conflicts with this one. For instance, let's say two participants, Bob and Chloe, make a claim almost simultaneously, in the same epoch. This would lead to both being present in the live group, and both \emph{appearing} valid against the latest epoch, because they are: it is only with reference to each other that they are not. The solution to this, however, is surprisingly simple, provided one has trust in the conflict resolution mechanisms described earlier. According to those, if conflicting epochs are created that have conflicting entries for a particular contract, one of the epochs will have its contents booted out into the live group, and those entries should then all be revalidated before being re-propagated. This makes consensus resolution a somewhat slow process, but this is necessary in order to exile one of the conflicting entries. The randomness of this highlights an essential principle in contract design: \textbf{if two contract entries conflict, the one that will prevail will most likely be entirely random}.

However, this still doesn't solve the problem of two conflicting entries in the live group, but this is even simpler: rather than using the state of the ledger up to \(\mathbb{E}_1\) to derive the state for the contract, use the state all the way up to what is known of the live group. This highlights another principle of contract design: \textbf{any conflicts between entries must be mutual, and orderless within a single epoch}. That is, a conflict should not be only caused if entries appear in a certain order, and, if entry A conflicts with entry B, entry B will implicitly conflict with entry A, leading to \emph{both} entries being removed from the live group --- this is what noting the entry that caused an error enables. There is the possibility of \emph{reverse propagation} here though, in which, perhaps through an offline node, or a resubmission, or any number of other methods, one of the entries that was removed is somehow resurrected and shows up again in the network. If the other one does not, then it will be considered valid on its own: hence, once again, conflict resolution is random within contracts --- for this reason, some contracts may wish to implement obscurity measures (e.g. the addressing system's use of encrypted primary entries, followed by a decrypting secondary entry).

Through this system, we can confidently say two things: the state of the ledger for all epochs before \(\mathbb{E}_\Gamma\) is surely valid, and the state of the live group and volatile epochs of all individual nodes will also be valid. The live groups that different nodes store probably will not be the same, due to the ongoing nature of propagation, but they will at least all be valid, in their various ways. Since conflict resolution is well-defined in Miriar, this creates a full system of arbitrary code execution in propagation validation.

One critical difference between Miriar and other smart contract systems, such as Ethereum, is that Miriar does not have an underlying 'gas' system, or any such transaction system that pays for contracts. Rather, any cryptocurrency infrastructure would be built on top of this, meaning the underlying system is far more resilient to exchange rate changes and other economic factors\footnote{Note that the incentive mechanisms described earlier do require some kind of payment infrastructure, but there is no need for this to run on the Miriar network itself, and hence it is often excluded from these kinds of discussions, due to the uniquely specific role it plays.}.

However, the problem of computational complexity must be addressed. Since contracts are executed in a containerised environment, and since they must be deterministic, they have no access to the network or any other system facilities. Any access to the Kolaris network on which they run should be facilitated by the nodes they are first submitted to, and the proofs thereof should be carried through the network with the entry: in such cases, the \emph{generated data} of an entry will be present, signed by the original node through which the entry entered the network (multiple can be used, but the one with the numerically largest signature will be favoured eventually). The agreement signatures of all other nodes are implicit, since they have continued the propagation. Such additional data is denoted as \(\eta\) in the entry payload described in Eq. (3.3):

\begin{align*}
\eta = \mathrm{sign}_{G_{SK}}(\{ \eta_0, \eta_1, \ldots, \eta_n, G_{PK} \}) \tag{8.1}
\end{align*}

for the entry node \(G\) and \(n\) proofs that had to be fetched from Kolaris. Note that these are \emph{perfect} for use in oracles (especially when combined with ZK-SNARKs). Importantly, if there are costs associated with these proofs (e.g. for computation), the entry node should also be passed a transaction entry, or the escrow of one. (That transaction should, however, not be dropped even if the smart contract later has a conflict or is invalidated, since the work has already been performed --- the mechanism behind this will be discussed later.)

As touched on earlier, every entry requires a minor proof of work \(\tau\) to prevent the network being spammed, though more complex contracts should have a more complex proof of work attached to place a slightly higher bar on exacting computation from network nodes. Further, a final restriction is placed on propagation computations: they must be able to be executed within a certain amount of time. If this fails, or if they breach other restrictions on CPU usage etc., then they should be terminated, and propagation should be failed. In such cases, nodes that cancel a validation should propagate the entry with a signature of it with the term \texttt{-c}, indicating it was cancelled. If a node receiving such an entry finds that the last three layers of signatures include \texttt{-c}, it should drop the entry entirely. At the discretion of networks, these resource limits should be minimum limits, and nodes should be able to individually set more lax restrictions, allowing the network to work better when more nodes have greater computation power.

For contracts that require more complex executions, they should defer to the Kolaris computation platform, which does have access of course to the state of the ledger up to \(\mathbb{E}_\Gamma\), the most recent non-volatile epoch, but such contracts should be careful that any conflicts are clearly raised by the contract logic itself, not this kind of pre-logic, since it does not have access to the volatile epochs and live group, where conflicts may be raised. In cases of mismanagement like this, contract states may become inconsistent, which can lead to the loss of real value, or, in catastrophic instances, major failures, such as false positive actions (e.g. if a smart contract controlled a missile launch system, you can see how this could go wrong with the incorrect parameters). Generally, \textbf{contracts must not assume that the network is wrong in the case of a state inconsistency in the ledger history}, since it is almost guaranteed to be a problem in their own logic. For instance, a missile control system should not assume that the network has been hacked by a foreign power and launch, rather it should assume that it is faulty, and enter a broken state. Where this occurs, the contract should be replaced, or, where it is deliberately possible, a key may perhaps be created and split by Shamir secret sharing between stakeholders to allow the state to be imperatively reset with a special kind of command entry. This is of course a semi-centralised paradigm, and it should be avoided in all cases but those in which it is essential.

With this system now described, we have provided the foundations on which Miriar functions: addressing, routing, currencies, and custom smart contracts are all implemented through custom validation logic (with the inbuilt and most popular ones being cached by nodes). This in turn means that updates to, say, the logic used to validate addresses in Miriar can be made without the need for updating Miriar itself: only the contract logic needs to be updated, which will necessarily change the hash, meaning the old logic can still be used if necessary, allowing a more gentle migration process deliberately reminiscent of Lykros' key turnover process.

\section{Integrating with Kolaris}

One of the great assets of Miriar is that it runs directly on a Kolaris network, giving it access to a structured gossip protocol that allows it to circumvent a number of other consensus needs. However, Miriar can also benefit from Kolaris by directly integrating with it, and using it for storage and computation where typical contracts are insufficient.

For instance, let's say a contract needs to retrieve some data from Kolaris, which is large enough that it could not be feasibly stored directly on the Miriar ledger. While this may sound like a very simple use-case, it involves a number of complexities, principally in terms of replicability: if the relevant Kolaris node's version changes during the entry's propagation, then some nodes will access one thing, and others another. Further, any queries that are made during the validation process must be executed by \emph{every single node}, significantly slowing down validation times and thereby making the network sluggish.

These problems can be solved through the aforementioned \(\eta\) parameter, which is attached to all ledger entries, but is outside the signature by the user who initially submitted the entry. This allows it to be populated by the first node involved in propagation, who may charge fees for this work arbitrarily: if, for instance, this initial work involves lodging a verifiable computation, the payment for that should be made through a separate transaction. In essence, this strategy allows contract validation to be an entirely offline endeavour, requiring no connection to the network to work properly. For there to be a wide gamut of possibilities in terms of integrating with Kolaris, several proof mechanisms are required:

\begin{enumerate}
\item{Proof of computation;}
\item{Proof of correct index following;}
\item{Proof of security;}
\item{And proof of version-completeness.}
\end{enumerate}

\emph{Despite their names, these proofs are used for cryptographic integrity, and have absolutely nothing whatsoever to do with mechanisms like proof of work and proof of stake.}

The first and third of these are the simplest to resolve: verifiable computation proofs are handled entirely by the Kolaris specification, and they can be included in the \(\eta\) parameter to enable simple offline validation that a computation was executed properly. Through the computation bridge system explained in that paper, this can be done without having to even trust the author of the computation (i.e. the node to whom the user first propagated). The third of these proofs relates to ensuring that a certain storage item has not been tampered with, which is guaranteed by the use of Lykros: provided the ID of the unit in question is known, it can be infallibly verified that only those with the correct permissions have accessed a certain unit. These proofs consist of storing the signatures involved, together with the derivation chains involved as well. For those unfamiliar, derivation chains are Lykros' mechanism of proving not only that a certain user had access to a certain key, but also that they were \emph{supposed} to have access to it, which enables the cryptographic protection of sharing. Verifying these chains is generally trivial for units that do not contain deeply nested prerequisites, but those that do may benefit from the chain memoization mechanisms described in the Lykros paper --- further discussion of such mechanisms is beyond the scope of this paper.

The final proof is also fairly easy to develop: if we take the unique identifier of a unit on the Kolaris network, that will include the \emph{unit key fingerprint}, which makes certain that we know all the keys are correct (see the Lykros paper), but it will \emph{not} allow us to know what the latest version is: for that, we have to query the nodes storing this unit. Note that, if a version is already known, this is not required unless the single node we do query does not know about the version we want, allowing these kinds of requests to be expedited in many instances. If we don't know what the latest version is, the typical method of finding out is to ask each of the PLRs, and go with the latest one (all versions are verifiable, since they are cryptographically signed by their creators), so we can create a proof of version completeness by simply collating the attestation signatures of all the PLRs involved.

It is the second proof that is the most complex: Kolaris uses special types of units called \emph{index units} to store details about other units. For example, if we were to decentralise the entirety of Wikipedia, it would need to be spread across many, many units, due to Kolaris' storage limits. Linking them all together would require units devoted to telling you where things are, but these units can also tell you what the latest version is and what the communication address is of the node storing them --- in sum, these details allow one to follow a link from an index unit instantly, without needing the Kolaris routing algorithm or needing to check with PLRs to make sure one has the latest version. If problems occur (e.g. the node storing the data has left the network), the usual mechanisms can be used as necessary to reconstitute where the data in question will now be. Very often, a Miriar contract will find a certain unit by using these indices, allowing contract developers to depend on content that may move around without worrying about the breakage of their own systems (they simply depend on the relevant index units). However, to validate that we indeed got to the correct unit, we would have to prove that we followed the index correctly.

Such a proof is out of the scope of the Kolaris specification, and is therefore described here: what the Kolaris paper describes to be the contents of index units (i.e. large lists with search terms and unique element identifiers mapping to unit IDs, communication addresses, and version numbers), we should construct a Merkle tree from, placing it in a plaintext metadata property (since having a contract decrypt an encrypted index is pointless: every node on the network would have to know the key) --- recall that these still have signatures for any modification to them. This allows a proof to be efficiently constructed that a certain row was indeed in the index, and allowing the usual Lykros security proofs to be included on the Merkle tree contents.

The only problem with this system is how the contract interacts with the index: typically, Kolaris allows the keys indices use to be totally freeform: they can be keywords, hashes of tags, bitwise search keys, or anything else. However, this means that a single search that a contract automatically performs might return multiple results: a Merkle proof could be constructed to say that the one the contract author \emph{intended} exists, while the other one is followed --- since the whole point of using an index unit is that you don't know where the data you want are actually being stored, this poses a distinct problem. To solve this, we make use of the requirement that all indices must have distinct element IDs, which are intended for automated searching. Importantly, these IDs cannot be deterministic: the whole point of an index is that you can have one ID and change the unit it points to later, if need be. If the element ID is hardcoded into the contract, then this provides all we need, \emph{unless} there are duplicate element IDs. This is a very specific problem though, and one that is quite difficult to practically replicate: it would require an attacker to be able to manipulate the element IDs in the first place, or to be in collusion with someone else who can, which defeats the entire purpose, since they could simply reassign the element ID to whatever else they want. That said, duplicate element IDs are a problem, and PLR signatures should be collated within index units that point to other index units, attesting that there are indeed no duplicate element IDs in the pointee.

Finally, one may wonder how a contract would resolve a human-readable address: this is generally not recommended, due to how easily changeable these are. While possible, it is recommended that RIU identifiers be hardcoded into contracts, to prevent a malicious RIU from taking over through legitimate payment at any moment. (While Merkle exclusion proofs could be employed here, there is little utility in doing so when a RIU can be trivially hardcoded.)

With all this done by the first node, any nodes who receive this \(\eta\) value will be able to validate all its components: any computation results, storage values, versions, and index followings are guaranteed, meaning these operations only have to be performed by that one node. However, there is still the problem of how we ensure the reciprocity of the computation of \(\eta\) with the propagation of the transaction to that node, compensating them for their efforts. If we use Miriar's \emph{bundling} mechanism (described later), the two could only be added to the ledger together, but this isn't quite what we want: ideally, the compensatory transaction should go through \emph{regardless} of whether or not the original entry was accepted later on, and it should only \emph{not} go through if the computation of \(\eta\) was not performed.

Imagine Alice wishes to submit some entry to the ledger which involves a substantial value of \(\eta\), which she would like to have computed for her for speed and automation. She would first find a node who charges an acceptable surcharge on on \(\eta\) computation (there would likely be a vibrant competitive market in this, since Alice only needs to pick a single node for this), and then she would submit her entry, as in Eq. (3.3), to this node, Bob, along with another entry that submits a payment to Bob of a value they agree on (the price of the computation itself, plus the surcharge), however, the latter entry will also require an \(\eta\) value, of the form

\begin{align*}
\eta' = \{ v, \eta, \eta_{s_1}, \ldots, \eta_{s_Q} \}, \tag{8.1.1}
\end{align*}

which is the original \(\eta\) value, together with the attestation signatures of a PLR quorum, who are required to verify that not only did Bob compute the proofs Alice required, but that he also stored them in a certain unit which she specified. Here, \(v\) is the contract information regarding the original contract of Alice's (which should include in its instance metadata the unit ID of Alice's selected unit), allowing nodes to verify that the \(\eta\) Bob submits here was of the correct form for Alice's requirements.

But why should Bob submit the proofs he creates to a unit on Kolaris? The answer is to get around the problem of single-node propagation: Alice cannot reasonably trust that Bob will not just compute what she wants, propagate her payment, and then discard the actual entry she wanted to send through to the network. Normally, this problem of censorship would be solved by Alice simply providing her entry to many nodes, who all begin propagation simultaneously, but, since we only want \(\eta\) to be computed once (as it is verifiable in its own right, and potentially costly to Alice to compute many times), we cannot reasonably take this approach. If, however, Alice designates a write-deadly\footnote{In Lykros, this refers to a unit that, once written to, cannot be changed, as it becomes \emph{deadlocked}.} unit for Bob to submit \(\eta\) to, and if she then tells a number of other nodes to look out for when that unit is populated, they can check this every few minutes, and, when they see the \(\eta\) value, they can continue propagation. This way, none of these nodes need to know about each other, and there can be as many of them as Alice likes, meaning collusion to censor Alice's entry becomes effectively impossible. If Bob were to transmit \(\eta\) directly to a series of nodes Alice designated, he could trivially choose not to do this (or, worse, bribe them to tell her that all is well).

Importantly, one may wonder how the attestation signatures of the PLRs of this unit can be trusted, since Bob knows exactly which nodes these are, and he could feasibly bribe them to produce such signatures, allowing the transaction to himself to go through without ever propagating Alice's entry. Although this would be subject to the extreme difficulties of bribery in Kolaris (related to the Byzantine Generals' Problem), this is entirely irrelevant here, since, no matter what, Bob must perform the computation of \(\eta\) (as he actually needs to include this directly in the transaction to him), meaning that becomes a fixed cost for him. The transaction will allow him to recoup this value, plus his surcharge, but, if he were to bribe the PLRs to allow him to censor Alice's entry, he would almost certainly be losing money, especially since surcharges will likely be driven close to zero in a mature, competitive network. Although censorship is theoretically possible here, there is an active disincentive against it, meaning this only becomes a concern in cases of censors operating large volumes of nodes and attempting to gradually bankrupt those whose entries they wish to censor. Due to the difficulty of controlling any meaningful portion of the Kolaris network, this problem is effectively moot in mature networks.

Importantly, all this exists solely for the purposes of automation, and, in many cases, it would make better sense for Alice to simply compute \(\eta\) herself, since, once again, it is totally per-se verifiable. Having the ability for contracts to automatically interoperate with the network, however, is certainly attractive, and may enable more advanced use-cases in the future.

Finally, the unit Alice asks Bob to submit his data to need only be paid for once, and this could either be an ephemeral unit for this single contract, or one Alice uses for many contracts, since she can tell the nodes propagating her entry which version to observe. Although it may seem as though Alice herself could exploit this to provide ancient values of \(\eta\) for a current contract, this is a matter for the contract itself, and, since all signatures include timestamps, it would be fairly trivial to reject such old entries (provided the contract also has access to the timestamps of epochs, especially \(\mathbb{E}_\Psi\)), which is the oldest epoch that can be validly signed, and hence it will be associated with the oldest valid timestamp for an \(\eta\) proof that needs to be `current'. Again, however, the permissible age of \(\eta\) proofs is an application-level concern.

\chapter{Sharding}
\label{sec:orga7ad456}

Having described Miriar's conflict resolution and propagation systems, along with its mechanisms to incentivise nodes to store the ledger, it may have become apparent that there is one thing that could greatly improve the ledger's performance: \emph{sharding}. This is the practice of splitting the ledger up into \emph{shards} across the network, thereby reducing the amount each individual node has to store from \(x\) to \(\frac{x}{Z}\), where \(Z\) is the number of shards used.

Before we can understand why sharding may be so beneficial for ledger systems like Miriar, we first need to understand exactly how we might \emph{shard} the ledger. The nave approach would be to simply split it into chunks, stored by different sections of the network, however this would remove all incentives for nodes to store their section, since only the section receiving new entries would have any possibility of getting rewards, rapidly leading to data loss and disengagement. Alternately, we could split the entire ledger into separate sub-ledgers, effectively creating multiple Miriar networks within the one. This way, new entries would be assigned to one subledger or another deterministically, in a similar way to how units are assigned to points in Kolaris' network space. However, this too has a problem: it does not account for the fact that PLRs are the central elements of the propagation set: if one were to divide the network, one wouldn have to do it along the lines of PLRs. Since PLRs do not cluster with each other, but are rather deliberately spread out according to the redundancy algorithm, this would be \emph{extremely} fragile and vulnerable to churn.

A much simpler solution reminiscent of the latter option is to use the Kolaris subnetwork system, whereby whole Kolaris networks can be created within the one network, while maintaining linkage through NLRs. This way, each shard/subledger would have its own subnetwork in Kolaris, which it would use to operate a normal Miriar ledger. PLRs would operate within these shards, although NLRs are a problem: if it is mandated of the host Kolaris network that all nodes register with either storage or computation and then one shard, every NLR will be on \emph{a} shard, but it just might not be the same shard as the node in question. However, provided \(C\) is high enough, there is enough redundancy in the system to tolerate this already, and, by using simpler shards with fewer nodes, conflicts are reduced, meaning it would be fine to propagate only to those NLRs which are on the same shard.

Importantly, in Kolaris networks using Miriar, all nodes must be part of a shard, which should be assigned with a separate addition certificate, as with any other subnetwork, but one that should declare the subnetwork as \texttt{miriar}, with the actual shard being determined by the hash of the addition certificate. Accepting this would require only minor modifications to the routing algorithm.

For every entry \(\epsilon\) that arrives on the network, its hash \(H(\epsilon)\) should be used to assign it to a bucket: if there are three shards, then an entry should be assigned to the third shard if its hash is numerically in the highest third of the hash-space. Note that, unlike other subnetworks in large-scale Kolaris networks that allow users to create their own, Miriar shards should be \emph{transparent subnetworks}, meaning the fact that there are multiple for the key \texttt{miriar} is acknowledged implicitly in the protocol. Of course, the nodes to whom entries are first given should be on the correct shard, or they should return an error and not propagate at all.

With all that described, we can now explore the exact benefits of sharding in this way. First, conflicts are reduced, since propagation is much simpler, thereby reducing the need to set \(\Gamma\) to higher values, and improving the speed with which entries can enter confirmed epochs. Secondly, and primarily, there is the reduction in the amount that nodes have to store by \(\frac{100}{Z}\%\), making it easier to participate in the network: even having just two shards would halve the amount that every node stores of the ledger. Importantly, however, higher numbers of shards have the distinct \emph{disadvantage} of having so few nodes that they end up with less stringent validation, potentially compromising the security of that shard, and thereby of the whole ledger.

Note that, while it may seem tempting to allocate a certain type of entry to a certain shard, this is pedantry, and should be rejected, since it will simply reduce the compexity of attacks by ensuring that, for example, to compromise a large number of addresses, all attackers need to do is take control of one shard, rather than many. On the point of attacks, it may seem as though our system for allocating entries to shards is highly vulnerable to being brute-forced, but one ought to remember that there is still a \(\tau\) in each \(\epsilon\), meaning a proof of work or the like would have to be constructed for every single one of these, making such an endeavour substantially more difficult.

To this point, entries for the same contract will very likely end up in completely different shards, meaning that, to get an accurate picture of the ledger, a client must launch at least \(Z\) lots of some number of requests that satisfy that client that the results they are receiving are adequate. This is another way in which a large number of shards is highly disadvantageous.

Finally, shards are a flexible system, since, once entries are validated, they can easily move into a different shard if, say, a new shard is added to the network, or an old one removed, since shard allocation is purely based on the number of shards. Having the number of shards change with network growth, up to a point, is probably a good idea, although further mathematical modelling should be devoted to this question.

\chapter{Cryptocurrency}
\label{sec:org7b7ddbf}

To begin this section, one thing should be made very clear: if this is the section of this paper you have been waiting for, you may well be disappointed. Miriar is not a 'DeFi' or 'Web3' project, it is an append-only distributed ledger extension on Kolaris --- make of that what you will. It is not intended to host some native 'Miriar coin' which can be mined, and it is not intended as a get-rich-quick scheme. For that reason, it is highly recommended that Miriar be deployed \emph{without} a native coin at all, and that instead the epoch rewards be lodged through an external cryptocurrency, such as Monero. The greatest value of Miriar comes in its ability to extend Kolaris for smart contracts, and in its refinements to the routing system, together with its ability to reliably map human-readable addresses to units on a Kolaris network.

With that said, a running a cryptocurrency on top of Miriar is perfectly viable, since the existing contract logic can be used very simply. Contracts have \emph{instances}\footnote{How contract instances depends greatly on the contract in question. For example, an escrow contract would have a different instance for every escrow, which would not apply in to a cryptocurrency, as discussed above. All this information should be encoded in the \(v\) parameter (see Eq. (3.3)).}, and therefore a contract can be established in which each instance of it is a single account (or 'wallet' in common cryptocurrency parlance), which can hold a certain balance (the emergent state of the contract).

To illustrate this, let's take the simplest example imaginable: Alice pays Bob ten tokens through this system. To do so, she would send an entry into the correct shard of the network that states that she wishes to pay Bob this amount. The validation logic for this kind of contract is very simple: if Alice's account has enough tokens in it to make this transaction, it should be considered valid. This means that, if Alice only has those 10 tokens, and she tries to spend them to both Bob and Chloe at the same time, these entries will conflict in the live group, and they will both be cancelled (though one may perhaps prevail randomly). This solves the double-spend problem immediately. If Alice spends all her tokens and then later tries to spend more, there will of course be a conflict with an existing epoch, which will invalidate the new entry.

Of course, in reality, most transactions are made up of \emph{exchanges}, in which two things of value are transferred between parties. If only one of these things exists on the network (i.e. the tokens), and the other is physical (e.g. a hotel room), then it would make sense for the transaction made to be one-way (unless the hotel room key is represented as a token with fixed ownership, however). In another exchange, however, in which Chloe pays Bob 15 tokens for a certain human-readable address he controls, Chloe would create an entry making the payment to Bob, and Bob would make an entry transferring his address to Chloe, but these two should only be executed if both pass. For instance, if Chloe makes another transaction that empties her account, and her transaction to Bob fails, the address should not be transferred. Likewise, if there is some problem with the address transfer, Chloe's transaction should be terminated. This can be achieved through \emph{bundling}, whereby multiple entries are grouped into the same super-entry, and they are then propagated together. Entries may be propagated in a bundle by adding a note to their \(\phi\) values that declare that they should only be considered valid if they are in a bundle with another entry with some specific entry ID (this is effectively equivalent to an \(\eta\) proof that an entry is already on the ledger, in advance of its being actually added). Therefore, Alice and Bob should generate the IDs of their entries, and exchange the final entries, before finalising and signing them, and then exchanging them. By including this reference to bundling in the payload, Bob cannot, for example, send Chloe's transaction through the network himself after she has validated it, without also sending his own through.

The problem with this system is that Bob could, of course, provide the ID of a completely different entry, perhaps one that simply transfers some tokens to himself, and Chloe would be none the wiser, leading to Bob getting Chloe's tokens without handing over his address, but this is solved by the intricacies of the entry ID system. Rather than being purely random, IDs should be the attachment of random data to the hash of the payload, such that

\begin{align*}
\epsilon_I = \{ H(\{ v, H(\mathbb{E}_2), \phi \}), \epsilon_\star \}, \tag{10.1}
\end{align*}

where \(\epsilon^\star\) is that random data. Note the exclusion of \(\tau\), since that should be made on the entire entry, including the UUID. Hence, rather than exchanging their IDs, Bob and Chloe should simply exchange the set \(\{ v, H(\mathbb{E}_2), \phi \}\), such that the IDs of the entries can be derived, and a bundle condition \(\beta\) can be added to the real payload (these are excluded from \(\epsilon_I\) itself, since they are not known until \emph{after} the IDs have been derived).

The final issue with this is that it assumes the entry payloads in question cannot be created without the signature of the intended party. In cases such as transactions, where a transaction can only be valid from Alice to Bob if it has Alice's signature, this is fine, however there may be other contract types in which this is not acceptable, and, in these cases, bundling will not be effective (escrow systems may be more suitable here). Hence, exchanges can easily be made through Miriar.

Importantly, although exchanges work nicely for purely digital transactions, the majority of transactions in practical use will have some anchoring in the physical world, and, in these cases, greater protections are needed. One of the properties of the Miriar live group is that it enables the cancellation of transactions very efficiently: one simply needs to submit a conflicting transaction that would empty one's own account, and the two will likely be both deleted from the live group. Of course, this is a misuse of Miriar's conflict resolution systems, and is therefore not to be relied on for actual transaction cancellation, but it does pose a problem for those wishing to ensure they will in fact be paid: what is to stop a conflict from getting in the way? In such cases, escrow systems are best, in which funds are held by a contract, and then released with a signature from the original payer. For example, let's say Alice wishes to order dinner and pick it up from a restaurant to take back home. She might pay for this dinner using a cryptocurrency running on Miriar, but she obviously will not wish to actually pay until she actually has her dinner. Similarly though, the restaurant will likely not be willing to simply accept her payment on the spot, not with Miriar, since there will be a window during which the entry may fail to go through. One may think this could be solved by Alice providing the entry itself to the restaurant (such that they could resubmit it if necessary), but this ignores the possibility of Alice misusing the conflict resolution mechanism. The only solution here is to somehow earmark some funds of Alice's for this payment, which she can reclaim, or submit to the restaurant. This can be done by Alice's transferring funds into an escrow contract, which will allow a reclaiming entry after, perhaps, 24 hours, allowing Alice to recoup her funds if need be, or a signature can be produced by Alice that would release these funds to their pre-determined destination: the restaurant. Then, Alice could simply load this release signature onto her phone, and provide it to the restaurant, who can then be assured that they have those funds: all they need to do is propagate it through Miriar, and they will be paid (resubmitting \emph{that} as necessary if there are unrelated conflicts). Alice has her dinner, and the restaurant has its compensation. In such cases of exchanges of digital assets being tied to the real world, escrow services are extremely useful (and fairly simple to build). While this does not fall into the Miriar protocol itself, it will be a foundational application-level system upon any large Miriar network, almost certainly.

Notably, transactions of currency are very common in contracts, for instance, in the earlier example of Alice's problem-solving smart contract, she paid out an amount of tokens to the winning participant, or reclaimed them herself if no one solved the problem in time. At that point, we simply called these tokens of pride, in effect, since they had no value, but now we could indeed link them to a cryptocurrency system. To do this, however, there would need to be two layers of contract validation: first the validation of the validity of the transaction, and second the validation of the validity of Alice's contract. To make this work, we define \emph{compound contracts}, which are simply contracts made up of multiple other contracts that call their logic explicitly and in a signed manner. Rather than re-implementing the currency logic, Alice should call out to it directly, and, since this can be validated, others can say with confidence that, even though the contract was written by Alice, it was using valid cryptocurrency rules, and therefore the token transfer is valid as well. Such compound contracts are actually surprisingly simple, but they solve a large number of problems and enable an even larger number of use-cases.

Astute readers may note that the proofs of work used to prevent spam in Miriar may be problematic in a cryptocurrency system. Since there are no transaction fees, which these proofs stand in for, it is entirely possible for one to transact an amount of tokens such that the proof of work is more costly than the transation itself, which may be problematic in \emph{some} applications. A simple solution to this is to allow bundles to have single proofs of work for many entries, allowing services to arise that will bundle micro-transactions and propagate them onto the ledger, reducing user costs. Again, however, this would likely be a fairly specialised use-case.

\part{Conclusion}
\label{sec:orgce41842}

To conclude, Miriar provides a new layer to the existing Quantorium stack of Lykros running on a Kolaris network, extending these existing technologies with an append-only ledger that provides new possibilities for mass data replication and micro-computation, facilitating things like smart contracts to be written and executed natively on a Kolaris network. On these foundations, a new hybrid routing system can be built for Kolaris, which prevents the domino attacks that pose a significant threat to the original protocol, while greatly improving the security and reliability of routing. Further, through this ledger system, an infallible map can be built of human-readable addresses to machine addresses on a Kolaris network, enabling the development of apps on Kolaris that can be identified and accessed through simple strings like \texttt{google.com}. Beyond these addressing and routing capabilities, a ledger of this kind has the potential to fully replace existing blockchain systems by enabling the development of efficient and verifiable smart contracts, which can form a cryptocurrency with no native base currency, allowing greater network resilience to price shocks, while maintaining the security promises that only cryptocurrencies can provide.

Through these technologies, Miriar provides the perfect complement to Lykros and Kolaris, allowing the network to effectively act as a single, unified platform for the decentralisation of technology and infrastructure. The development of smart contracts originally, through the Ethereum blockchain, has led to an explosion of innovation in the sector of decentralised finance in particular, but, with access to a native mass storage and computation layer with the verifiability promises of Kolaris and the cryptographic security of Lykros, it is our firm belief that these systems can be improved by no small amount. Kolaris and Lykros on their own provide an incredible backend system for all number of things, and Miriar not only improves them for their original tasks, but also extends them greatly by allowing an entirely separate class of operation --- that which requires a universally replicated ledger and arbtirary propagation validation operations --- to be executed natively on the network, without reference to external systems.

Finally, in terms particularly of cryptocurrency, Miriar pioneers a new kind of ledger system through the unique deterministic propagation architecture enabled by Kolaris: unlike existing blockchain systems, Kolaris has a clear system by which data should be sent between nodes, allowing conflicts to be more gracefully and effectively handled, removing the need for expensive consensus mechanisms such as proof of work, thereby reducing the network's carbon footprint greatly, a critical requirement of new decentralised technology in an age of human-accelerated global warming. Further, by repurposing proof of work for its original intended purpose of preventing spam, Miriar is able to minimise conflicts while avoiding the need for a so-called 'gas fee', or in fact any native cryptocurrency --- this makes Miriar the most flexible decentralised ledger platform yet.

As stated throughout the documents published by the Quantorium project, our aim is very simple: to improve the human race as much as possible by pioneering novel distributed technologies to enable the next generation of development to benefit the people of the human race, beyond just governments and corporations. There can be no doubt that blockchain will continue to exist for the forseeable future, simply because of its incredible usage, but, hopefully, Miriar can learn further from such existing technologies, particularly in the realm of contract optimisation, and, conversely, we sincerely hope that existing technologies can learn from Miriar. As a species, it is high time to take control of our data and move forward into a world in which the incentives that govern our societies are geared for the betterment of \emph{the people}, something that has thus far proved largely impossible, time and time again. It is our hope that, through decentralisation, systems with such positive incentives may finally be able to be built, and therefore that they may, directly or indirectly, increase human prosperity. All that remains now is to use them.
\end{document}
